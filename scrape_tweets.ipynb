{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Tweets Scraping Notebook\nFollowing these tutorials: \n\n1. https://medium.com/@jcldinco/downloading-historical-tweets-using-tweet-ids-via-snscrape-and-tweepy-5f4ecbf19032\n2. https://towardsdatascience.com/how-to-scrape-more-information-from-tweets-on-twitter-44fd540b8a1f#7152",
      "metadata": {
        "tags": [],
        "cell_id": "00000-60755f37-3424-4342-a263-265323d56f5c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 0. Use snscrape to crawl tweet IDs with certain search creteria\nSocial network crawler snscrape: https://github.com/JustAnotherArchivist/snscrape (NOTE: snscrape requires Python 3.8 environment!)\n    \n    pip install snscrape\n\nTwitter advanced search criteria: https://github.com/igorbrigadir/twitter-advanced-search\n\nExample terminal comand to run:\n\n    snscrape twitter-search -v \"(#coronavirus OR #covid19 OR #covid-19 OR #covid) since:2020-05-01 until:2020-05-15 lang:en\" > ht_covid_0501_0515.txt\n\nNOTE: since Twitter has been phasing out geo-tagging, searching for geo-tagged tweets has been very limiting. Thus, we skip geo location for now and filter for tweets from US users in the tweets extraction step\n\nRun the following Python 3.8 script (snscrape_script.py) in terminal to automate snscrape for crawling a long period of tweet IDs:",
      "metadata": {
        "tags": [],
        "cell_id": "00002-a349db2e-80f4-473f-b1bd-d0ccba4c0601",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "    import subprocess\n    import datetime\n    import snscrape\n\n    if __name__ == \"__main__\":\n        # Edit start & end date here\n        start_date = datetime.date(2020, 5, 16)\n        end_date = datetime.date(2020, 11, 23)\n        delta = datetime.timedelta(days=15)\n\n        while start_date <= end_date:\n            since = start_date.strftime(\"%Y-%m-%d\")\n            until = (start_date + delta).strftime(\"%Y-%m-%d\");\n            fill = [since, until, since[-5:] + \"_\" + until[-5:]]\n            query = \"(#coronavirus OR #covid19 OR #covid-19 OR #covid) since:{0} until:{1} lang:en\".format(*fill)\n            cmd1 = \"snscrape --max-results 1000000 twitter-search\"\n            file = \"ht_covid_{2}.txt\".format(*fill)\n            print(datetime.datetime.now(), ' ', query) \n\n            # Redirect output to a .txt file       \n            with open(file, 'w') as f:\n                subprocess.run(cmd1.split() + [query], stdout=f, text=True)\n            start_date += delta",
      "metadata": {
        "tags": [],
        "cell_id": "00003-899212da-3230-4000-ae3e-df9a14b004cd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1. Tokens for Tweepy Authentication\nFill in your Twitter developer account API keys",
      "metadata": {
        "tags": [],
        "cell_id": "00002-70e0866a-08d2-4c32-9c99-642b05d9ef15",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-02072a72-5268-4cce-a646-4e6c57ecfc05",
        "output_cleared": false,
        "source_hash": "62f91f0",
        "execution_millis": 0,
        "execution_start": 1606896957335,
        "deepnote_cell_type": "code"
      },
      "source": "import pandas as pd\nimport tweepy\n\n# Moira's API keys\nconsumer_key = \"rnf9JPI67xFrwkhqzDKDvkNHw\"\nconsumer_secret = \"SruqMAC6yJxu5JVOiQiRfJWNl1V2aLLtQm4WVOvnQsGWxZb4Fe\"\n# bearer_token = \"AAAAAAAAAAAAAAAAAAAAAJrSJQEAAAAAdYm3S5f%2Fe3DLjNcRqZ7pSAe7MO8%3DsISkKVeuKuU4qtyniNFVBzHGdnm88rK2K0BONYWEkGrpMDqhrf\"\naccess_token = \"1324271022615326721-VZKJ4DgJK4CRcOVOyuGCQ3IFgRJbqd\"\naccess_token_secret = \"2mDKPt6RYDAkmplOxQLFHMjzWDX8zqhMUTUKdkCeSciv8\"\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\napi = tweepy.API(auth,wait_on_rate_limit=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 2. Tweet Content Scraping\nUse \"snscrape\" to scrape historical tweets & write urls to a txt file\n\nNote: For some reasons cloud services like Colab & Deepnote all block the snscrape module, so this step has to be done locally\n",
      "metadata": {
        "tags": [],
        "cell_id": "00004-21685f93-27d4-4d64-8aa7-851fd985f292",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00003-04273174-e9fb-4a22-bf61-1ed4dfe32deb",
        "output_cleared": false,
        "source_hash": "e80c4eb1",
        "execution_millis": 1155,
        "execution_start": 1606897066275,
        "deepnote_cell_type": "code"
      },
      "source": "# Name of the .txt file with tweets URLs to be extracted\nfilename = \"ht_covid_09-13_09-28\"\n\ntweet_url = pd.read_csv(filename + \".txt\", index_col= None, header = None, names = [\"links\"])\ntweet_url.head()",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "application/vnd.deepnote.dataframe.v2+json": {
              "row_count": 5,
              "column_count": 1,
              "columns": [
                {
                  "name": "links",
                  "dtype": "object",
                  "stats": {
                    "unique_count": 5,
                    "nan_count": 0,
                    "categories": [
                      {
                        "name": "https://twitter.com/iOptimizeRealty/status/1310368622196527104",
                        "count": 1
                      },
                      {
                        "name": "https://twitter.com/victori92735456/status/1310368618127847425",
                        "count": 1
                      },
                      {
                        "name": "3 others",
                        "count": 3
                      }
                    ]
                  }
                },
                {
                  "name": "_deepnote_index_column",
                  "dtype": "int64"
                }
              ],
              "rows_top": [
                {
                  "links": "https://twitter.com/iOptimizeRealty/status/1310368622196527104",
                  "_deepnote_index_column": 0
                },
                {
                  "links": "https://twitter.com/victori92735456/status/1310368618127847425",
                  "_deepnote_index_column": 1
                },
                {
                  "links": "https://twitter.com/Joel_Agius1/status/1310368585802342401",
                  "_deepnote_index_column": 2
                },
                {
                  "links": "https://twitter.com/unbrothodox/status/1310368582912667650",
                  "_deepnote_index_column": 3
                },
                {
                  "links": "https://twitter.com/albertan48/status/1310368573886529537",
                  "_deepnote_index_column": 4
                }
              ],
              "rows_bottom": null
            },
            "text/plain": "                                               links\n0  https://twitter.com/iOptimizeRealty/status/131...\n1  https://twitter.com/victori92735456/status/131...\n2  https://twitter.com/Joel_Agius1/status/1310368...\n3  https://twitter.com/unbrothodox/status/1310368...\n4  https://twitter.com/albertan48/status/13103685...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>links</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://twitter.com/iOptimizeRealty/status/131...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://twitter.com/victori92735456/status/131...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://twitter.com/Joel_Agius1/status/1310368...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://twitter.com/unbrothodox/status/1310368...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://twitter.com/albertan48/status/13103685...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-54c29313-75af-4390-bcb3-922ec762eb54",
        "output_cleared": false,
        "source_hash": "d82d3d9",
        "execution_millis": 13494,
        "execution_start": 1606897067859,
        "deepnote_cell_type": "code"
      },
      "source": "# extract tweet ID from url\naf = lambda x: x[\"links\"].split(\"/\")[-1]\ntweet_url['id'] = tweet_url.apply(af, axis=1)\nids = tweet_url['id'].tolist()\n\nbatch_size = 100\ntotal_count = len(ids)\nchunks = (total_count - 1) // batch_size + 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "created_at : The time the status was posted.\nid : The ID of the status.\nid_str : The ID of the status as a string.\ntext : The text of the status.\nentities : The parsed entities of the status such as hashtags, URLs etc.\nsource : The source of the status.\nsource_url : The URL of the source of the status.\nin_reply_to_status_id : The ID of the status being replied to.\nin_reply_to_status_id_str : The ID of the status being replied to in as a string.\nin_reply_to_user_id : The ID of the user being replied to.\nin_reply_to_user_id_str : The ID of the user being replied to as a string.\nin_reply_to_screen_name : The screen name of the user being replied to\nuser : The User object of the poster of the status.\ngeo : The geo object of the status.\ncoordinates : The coordinates of the status.\nplace : The place of the status.\ncontributors : The contributors of the status.\nis_quote_status : Indicates whether the status is a quoted status or not.\nretweet_count : The number of retweets of the status.\nfavorite_count : The number of likes of the status.\nfavorited : Indicates whether the status has been favourited by the authenticated user or not.\nretweeted : Indicates whether the status has been retweeted by the authenticated user or not.\npossibly_sensitive : Indicates whether the status is sensitive or not.\nlang : The language of the status.",
      "metadata": {
        "tags": [],
        "cell_id": "00007-85badac4-65f7-471e-9aa7-6393769cee06",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00005-21db31d9-852f-4b98-b8d1-66b23655e398",
        "output_cleared": false,
        "source_hash": "3bdbabf7",
        "execution_millis": 0,
        "execution_start": 1606897214219,
        "deepnote_cell_type": "code"
      },
      "source": "import time\n\ndef fetch_tw(ids, filename):\n    for attempt in range(16):\n        try:\n            list_of_tw_status = api.statuses_lookup(ids, tweet_mode= \"extended\")\n            #print('Looked up successfully')\n            break\n        except Exception as e:\n            print(e, 'Wait 1 min & retry...Attempt:', attempt)\n            time.sleep(60)\n        \n    tweet_df = pd.DataFrame()\n    for status in list_of_tw_status:\n        # place = 0\n        # #print(type(status))\n        # if status.place != None:\n        #     place = status.place.full_name\n        tweet_elem = {\"tweet_id\": status.id,\n                    \"screen_name\": status.user.screen_name,\n                    \"tweet\": status.full_text,\n                    \"date\": status.created_at,\n                    # \"language\": status.lang, # all English\n                    #\"place\": place,\n                    \"user_location\": status.user.location\n                    }\n        tweet_df = tweet_df.append(tweet_elem, ignore_index = True)\n    return tweet_df\n    #empty_data.to_csv(filename, mode=\"a\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Fetch tweets in batachs and write to a csv",
      "metadata": {
        "tags": [],
        "cell_id": "00006-5408c22c-ea32-4fe9-a67b-fd5e9e7323ff",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00007-fd0d5850-61c6-4ad0-b81f-065c2bc6b215",
        "output_cleared": false,
        "source_hash": "950d67b9",
        "execution_millis": 10647648,
        "execution_start": 1606897221891,
        "deepnote_cell_type": "code"
      },
      "source": "dataset = pd.DataFrame()\nfor i in range(chunks):\n        batch = ids[i*batch_size:(i+1)*batch_size]\n        dataset = dataset.append(fetch_tw(batch, filename), ignore_index=True)\n        #print('batch ', i, 'out of ', chunks)\n        \ndataset.to_csv(filename + \".csv\", mode=\"a\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-612b8d99-aa5a-45a3-8a7b-bdeed19da09e",
        "output_cleared": false,
        "source_hash": "e29bcd5e",
        "execution_millis": 29,
        "execution_start": 1607040251283,
        "deepnote_cell_type": "code"
      },
      "source": "# Extract content from tweet ID files (.txt)\nimport pandas as pd\nimport tweepy\nimport time\nimport extract_states\n\ndef tweetID2contents(filename, batch_size=100):\n\n    # Your Tweepy API keys\n    consumer_key = \"\"\n    consumer_secret = \"\"\n    bearer_token = \"\"\n    access_token = \"\"\n    access_token_secret = \"\"\n\n    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n    auth.set_access_token(access_token, access_token_secret)\n    api = tweepy.API(auth,wait_on_rate_limit=True)\n\n    def fetch_tw(ids, filename):\n        for attempt in range(16):\n            try:\n                list_of_tw_status = api.statuses_lookup(ids, tweet_mode= \"extended\")\n                #print('Looked up successfully')\n                break\n            except Exception as e:\n                print(e, 'Wait 1 min & retry...Attempt:', attempt)\n                time.sleep(60)\n            \n        tweet_df = pd.DataFrame()\n        for status in list_of_tw_status:\n            # place = 0\n            # #print(type(status))\n            # if status.place != None:\n            #     place = status.place.full_name\n            tweet_elem = {\"tweet_id\": status.id,\n                        \"screen_name\": status.user.screen_name,\n                        \"tweet\": status.full_text,\n                        \"date\": status.created_at,\n                        # \"language\": status.lang, # all English\n                        #\"place\": place,\n                        \"user_location\": status.user.location\n                        }\n            tweet_df = tweet_df.append(tweet_elem, ignore_index = True)\n        return tweet_df\n    \n    # extract tweet ID from url\n    tweet_url = pd.read_csv(filename + \".txt\", index_col= None, header = None, names = [\"links\"]) \n    af = lambda x: x[\"links\"].split(\"/\")[-1]\n    tweet_url['id'] = tweet_url.apply(af, axis=1)\n    ids = tweet_url['id'].tolist()\n\n    total_count = len(ids)\n    chunks = (total_count - 1) // batch_size + 1\n\n    # fetch tweeet contents & write to csv\n    dataset = pd.DataFrame()\n    for i in range(chunks):\n            batch = ids[i*batch_size:(i+1)*batch_size]\n            dataset = dataset.append(fetch_tw(batch, filename), ignore_index=True)\n            #print('batch ', i, 'out of ', chunks)\n            \n    dataset.to_csv(filename + \".csv\", mode=\"a\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-2b9a798a-4bb6-4ce8-a4aa-f1dbbbcf36e4",
        "output_cleared": false,
        "source_hash": "6dcc6793",
        "execution_millis": 32112432,
        "execution_start": 1607040352267,
        "deepnote_cell_type": "code"
      },
      "source": "# Automate extracting tweet files\nimport datetime\n\nstart_date = datetime.date(2020, 5, 1)\nend_date = datetime.date(2020, 6, 15)\ndelta = datetime.timedelta(days=15)\n\nwhile start_date < end_date:\n    since = start_date.strftime(\"%Y-%m-%d\")\n    until = (start_date + delta).strftime(\"%Y-%m-%d\");\n    fill = [since, until, since[-5:] + \"_\" + until[-5:]]\n    # Name of the .txt file with tweets URLs to be extracted\n    filename = \"ht_covid_{2}\".format(*fill)\n    print(datetime.datetime.now(), filename)\n    # Extract tweets\n    tweetID2contents(filename)\n\n    start_date += delta",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "2020-12-04 00:05:52.217886 ht_covid_05-01_05-16\n2020-12-04 03:06:12.327026 ht_covid_05-16_05-31\n2020-12-04 06:04:33.100475 ht_covid_05-31_06-15\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-4b581ef7-8286-4b59-a50e-d61c31e8c1c9",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=cfafe70e-b705-46d4-9ab8-a3eb30c9a7b3' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "ea3e80b4-b748-4412-8f1f-04ec6bcf60d8",
    "deepnote_execution_queue": [],
    "deepnote": {}
  }
}